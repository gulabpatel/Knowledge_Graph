{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01: KG_Embedding_Tutorial_AmpliGraph_Library.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "do2yT94Yagds",
        "HM0ax6rPpbpe",
        "ZTE3qGJ-qPcX",
        "1xP_BA1Gmto3"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulabpatel/Knowledge_Graph/blob/main/01%3A%20KG_Embedding_Tutorial_AmpliGraph_Library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTkVsbcKegOD"
      },
      "source": [
        "---\n",
        "# **Knowledge Graph Embeddings: From Theory to Practice**\n",
        "\n",
        "Video walkthrough the code : https://www.youtube.com/watch?v=gX_KHaU8ChI&t=8061s\n",
        "\n",
        "###(Hands-on Session)\n",
        "\n",
        "<u>**Contents**</u>\n",
        "- Loading a KG and creating train/test splits\n",
        "- Training and evaluating a KGE Model\n",
        "- Testing user hypothesis\n",
        "- Early stopping and types of evaluation\n",
        "- Choosing model hyperparameters\n",
        "- Discovering facts using trained model\n",
        "- Visualizing embeddings and Clustering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17mJcCLovIkx"
      },
      "source": [
        "# 1. Introduction and Preliminaries\n",
        "\n",
        "For this hands-on tutorial, we will be using the open-source library [AmpliGraph](https://github.com/Accenture/AmpliGraph).\n",
        "\n",
        "Let's start by installing the library and it's dependencies, and then importing the libraries used in this tutorial. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoUV52ke_3fz"
      },
      "source": [
        "# If running on local system execute this cell\n",
        "\n",
        "# Install CUDA\n",
        "#! conda install -y cudatoolkit=10.0\n",
        "\n",
        "# Install cudnn libraries\n",
        "# This library improves the performance of tensorflow, however, we need to give up determinism over speed.\n",
        "#! conda install cudnn=7.6\n",
        "\n",
        "# Install tensorflow GPU \n",
        "#! pip install tensorflow-gpu==1.15.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dcvCAfyDc5S"
      },
      "source": [
        "# If using Google Colab run this cell \n",
        "\n",
        "# select tensorflow version for colab \n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR0IriwbE63d"
      },
      "source": [
        "Let us check if tensorflow is correctly installed and if we can access the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs4xn9CWE5Yx"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "print('TensorFlow  version: {}'.format(tf.__version__))\n",
        "\n",
        "# Get the GPU name\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM6Awy5WFUVA"
      },
      "source": [
        "Let's install AmpliGraph and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgs8cTcu9hUM"
      },
      "source": [
        "%%capture \n",
        "# Install AmpliGraph library\n",
        "! pip install ampligraph\n",
        "\n",
        "# Required to visualize embeddings with tensorboard projector, comment out if not required!\n",
        "! pip install --user tensorboard\n",
        "\n",
        "# Required to plot text on embedding clusters, comment out if not required!\n",
        "! pip install --user git+https://github.com/Phlya/adjustText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqGjJ_SYFxIH"
      },
      "source": [
        "# All imports used in this tutorial \n",
        "%tensorflow_version 1.x\n",
        "import ampligraph\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from ampligraph.datasets import load_fb15k_237\n",
        "from ampligraph.evaluation import train_test_split_no_unseen, evaluate_performance, mr_score, mrr_score, hits_at_n_score\n",
        "from ampligraph.discovery import query_topn, discover_facts, find_clusters\n",
        "from ampligraph.latent_features import TransE, ComplEx, HolE, DistMult, ConvE, ConvKB\n",
        "from ampligraph.utils import save_model, restore_model\n",
        "\n",
        "def display_aggregate_metrics(ranks):\n",
        "    print('Mean Rank:', mr_score(ranks)) \n",
        "    print('Mean Reciprocal Rank:', mrr_score(ranks)) \n",
        "    print('Hits@1:', hits_at_n_score(ranks, 1))\n",
        "    print('Hits@10:', hits_at_n_score(ranks, 10))\n",
        "    print('Hits@100:', hits_at_n_score(ranks, 100))\n",
        "\n",
        "print('Ampligraph version: {}'.format(ampligraph.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2zQO7QqjYQW"
      },
      "source": [
        "---\n",
        "# 2. Loading a Knowledge Graph dataset\n",
        "\n",
        "To begin with we're going to need a knowledge graph, so let's load a standard knowledge graph called ***Freebase-15k-237***. \n",
        "\n",
        "Ampligraph provides a set of APIs to [load standard knowledge graphs](https://docs.ampligraph.org/en/1.3.1/ampligraph.datasets.html#benchmark-datasets-loaders). \n",
        "\n",
        "Also provided are a set of APIs load csv, ntriples and rdf formats. Details can be found [here](https://docs.ampligraph.org/en/1.3.1/ampligraph.datasets.html#loaders-for-custom-knowledge-graphs)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYLn3NXKegOm"
      },
      "source": [
        "from ampligraph.datasets import load_fb15k_237, load_wn18rr, load_yago3_10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKrD7K04egPS"
      },
      "source": [
        "\n",
        "For this tutorial we have remapped the IDs of freebase 237 and created a csv file containing human readable names instead of IDs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHzhvBhbegPX"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "URL = 'https://ampgraphenc.s3-eu-west-1.amazonaws.com/datasets/freebase-237-merged-and-remapped.csv'\n",
        "dataset = pd.read_csv(URL, header=None)\n",
        "dataset.columns = ['subject', 'predicate', 'object']\n",
        "dataset.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pgpcidHegQC"
      },
      "source": [
        "print('Total triples in the KG:', dataset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flR0xOXmegP9"
      },
      "source": [
        "\n",
        "![KG](https://user-images.githubusercontent.com/39597669/90747195-9fc44c80-e2c8-11ea-9f70-097993581bac.png) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQJkziMCegQL"
      },
      "source": [
        "\n",
        "## 2.1 Create training, validation and test splits\n",
        "\n",
        "Let's use the [`train_test_split_no_unseen`](https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.evaluation.train_test_split_no_unseen.html?#train-test-split-no-unseen) function provided by Ampligraph to create the training, validation and test splits. \n",
        "\n",
        "This API ensures that the test and validation splits contains triples whose entities are \"seen\" during training. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltijAdQtegQN"
      },
      "source": [
        "from ampligraph.evaluation import train_test_split_no_unseen\n",
        "# get the validation set of size 500\n",
        "test_train, X_valid = train_test_split_no_unseen(dataset.values, 500, seed=0)\n",
        "\n",
        "# get the test set of size 1000 from the remaining triples\n",
        "X_train, X_test = train_test_split_no_unseen(test_train, 1000, seed=0)\n",
        "\n",
        "print('Total triples:', dataset.shape)\n",
        "print('Size of train:', X_train.shape)\n",
        "print('Size of valid:', X_valid.shape)\n",
        "print('Size of test:', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q38cXnYHHI6"
      },
      "source": [
        "##**Key Takeaways**\n",
        "\n",
        "- `train_test_split_no_unseen` API can be used to generate train/test splits such that test set contains only entities 'seen' during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgAcQ1g3egQe"
      },
      "source": [
        "---\n",
        "# 3. Model Training\n",
        "Now that we have split the dataset, let's dive directly into model training. \n",
        "\n",
        "Let us create a TransE model and train it on the training split using the `fit` function.\n",
        "\n",
        "**TransE** is one of the first embedding models which set the platform for the KGE research. It uses simple vector algebra to score the triples. It has very low number of trainable parameters compared to most models. \n",
        "\n",
        "<center>$f = - || s + p - o ||_{n}$</center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap1Yd4LEegQg"
      },
      "source": [
        "from ampligraph.latent_features import TransE\n",
        "\n",
        "model = TransE(k=150,                                                             # embedding size\n",
        "               epochs=100,                                                        # Num of epochs\n",
        "               batches_count= 10,                                                 # Number of batches \n",
        "               eta=1,                                                             # number of corruptions to generate during training\n",
        "               loss='pairwise', loss_params={'margin': 1},                        # loss type and it's hyperparameters         \n",
        "               initializer='xavier', initializer_params={'uniform': False},       # initializer type and it's hyperparameters\n",
        "               regularizer='LP', regularizer_params= {'lambda': 0.001, 'p': 3},   # regularizer along with its hyperparameters\n",
        "               optimizer= 'adam', optimizer_params= {'lr': 0.001},                # optimizer to use along with its hyperparameters\n",
        "               seed= 0, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "from ampligraph.utils import save_model, restore_model\n",
        "save_model(model, 'TransE-small.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHtntuS1Jzf8"
      },
      "source": [
        "You can refer [this link](https://docs.ampligraph.org/en/latest/api.html) for detailed explaination of the parameters and their values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaJeVr-megQq"
      },
      "source": [
        "## 3.1 Compute the evaluation metrics\n",
        "\n",
        "### Per triple metrics:\n",
        "This is a metric that is computed for each test set triple:\n",
        "\n",
        "- **score**: This is the value assigned to a triple, by the model, by applying the scoring function.\n",
        "\n",
        "Let's look at how we can get the score for a triple of interest and how to interpret it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edjJcTReegQs"
      },
      "source": [
        "test_triple = ['harrison ford', \n",
        "               '/film/actor/film./film/performance/film', \n",
        "               'star wars']\n",
        "\n",
        "triple_score = model.predict(test_triple)\n",
        "\n",
        "print('Triple of interest:\\n', test_triple)\n",
        "print('Triple Score:\\n', triple_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQwswE4qFArO"
      },
      "source": [
        "But what does this score tell you? Nothing! It is just a value. In order to interpret the score we have 2 options:\n",
        "\n",
        "1. We can create a list of hypothesis that we want to test, score them and then choose the top n hypothesis as True statements.\n",
        "\n",
        "2. As described earlier in the theory section, unlike classification task, we are doing a learning to rank task. In order to interpret the score we can generate the corruptions and compare the triple score against the scores of corruptions to see how well does the model rank the test triple against them.\n",
        "\n",
        "\n",
        "Let's look at the first option. Let us create a list of hypothesis and score them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKewmQmp-1od"
      },
      "source": [
        "import numpy as np\n",
        "list_of_actors = ['salma hayek', 'carrie fisher', 'natalie portman',  'kristen bell',\n",
        "                  'mark hamill', 'neil patrick harris', 'harrison ford' ]\n",
        "\n",
        "# stack it horizontally to create s, p, o\n",
        "hypothesis = np.column_stack([list_of_actors, \n",
        "                              ['/film/actor/film./film/performance/film'] * len(list_of_actors),\n",
        "                              ['star wars'] * len(list_of_actors),\n",
        "                             ])\n",
        "\n",
        "# score the hypothesis\n",
        "triple_scores = model.predict(hypothesis)\n",
        "\n",
        "# append the scores column\n",
        "scored_hypothesis = np.column_stack([hypothesis, triple_scores])\n",
        "# sort by score in descending order\n",
        "scored_hypothesis = scored_hypothesis[np.argsort(scored_hypothesis[:, 3])]\n",
        "scored_hypothesis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UUNCR-p_EmC"
      },
      "source": [
        "\n",
        "- **rank**: For a triple, this metric is computed by generating corruptions and then scoring them and computing the rank(position) of the triple score against the corruptions. The pseudocode and the example illustrates how to compute rank on the test set.\n",
        "\n",
        "         for each test set triple <s, p, o>:\n",
        "                 a. Compute the score of the test triple (hypothesis) \n",
        "                     hypothesis_score = score(<s, p, o>)\n",
        "                     \n",
        "                 b. Generate the subject corruptions \n",
        "                         sub_corr = <?, p, o>\n",
        "                 c. Compute the score of the subject corruptions\n",
        "                         sub_corr_score = score(sub_corr) \n",
        "                 d. Find the position of hypothesis_score in sub_corr_score to get the sub_rank\n",
        "                   \n",
        "                 e. Generate the object corruption \n",
        "                         obj_corr = <s, p, ?>\n",
        "                 f. Compute the score of the object corruptions\n",
        "                         obj_corr_score = score(obj_corr) \n",
        "                 g. Find the position of hypothesis_score in obj_corr_score to get the obj_rank\n",
        "                 \n",
        "                 h. Return rank = [sub_rank, obj_rank]\n",
        "\n",
        "\n",
        "\n",
        "![rank example](https://user-images.githubusercontent.com/281477/90627614-14897f00-e214-11ea-8f8e-d57da9888606.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do2yT94Yagds"
      },
      "source": [
        "### Illustrative Example "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dImehz68LHQh"
      },
      "source": [
        "**Compute the score of the test triple**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpRVBVn_K-_S"
      },
      "source": [
        "test_triple = ['harrison ford', \n",
        "               '/film/actor/film./film/performance/film', \n",
        "               'star wars']\n",
        "\n",
        "triple_score = model.predict(test_triple)\n",
        "\n",
        "print('Triple of interest:\\n', test_triple)\n",
        "print('Triple Score:\\n', triple_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlLprqLE_Hby"
      },
      "source": [
        "Before generating the corruptions, let us look at the number of unique entities present in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMEBkOW1VoOS"
      },
      "source": [
        "print('The number of unique entities:', len(model.ent_to_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXBikfbWkuK6"
      },
      "source": [
        "**Generate the subject *corruptions* and compute rank**\n",
        "> ```sub_corr = <?, p, o>```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE6lw5FB_I_4"
      },
      "source": [
        "\n",
        "subj_corr =  np.column_stack([list(model.ent_to_idx.keys()),\n",
        "                [test_triple[1]] * len(model.ent_to_idx), \n",
        "                [test_triple[2]] * len(model.ent_to_idx)])\n",
        "\n",
        "print('Subject corruptions:\\n', subj_corr)\n",
        "print('\\nSize of subject corruptions:\\n', subj_corr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO1HXo4x7t2R"
      },
      "source": [
        "**Compute the score of the subject corruptions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N65INkWR_LFx"
      },
      "source": [
        "sub_corr_score = model.predict(subj_corr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu7vlKdg_OoM"
      },
      "source": [
        "Now that we have a score, let us compute the rank as follows:\n",
        "\n",
        "<center>$COUNT ( corruption_{score} >= triple_{score} )$</center>\n",
        "\n",
        "Find the position of hypothesis_score in sub_corr_score to get the sub_rank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V070u2N2_NBM"
      },
      "source": [
        "sub_rank_worst = np.sum(np.greater_equal(sub_corr_score, triple_score[0])) + 1\n",
        "\n",
        "print('Assigning the worst rank (to break ties):', sub_rank_worst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADOTNTGakqAO"
      },
      "source": [
        "**Generate the object *corruptions* and compute rank**\n",
        "\n",
        ">    ``` obj_corr = <s, p, ?> ```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MBRrCM0_RRB"
      },
      "source": [
        "obj_corr =  np.column_stack([\n",
        "                [test_triple[0]] * len(model.ent_to_idx),\n",
        "                [test_triple[1]] * len(model.ent_to_idx), \n",
        "                     list(model.ent_to_idx.keys())])\n",
        "\n",
        "\n",
        "print('Object corruptions:\\n', obj_corr)\n",
        "print('\\nSize of object corruptions:\\n', obj_corr.shape)\n",
        "\n",
        "# f. Compute the score of the object corruptions\n",
        "obj_corr_score = model.predict(obj_corr)\n",
        "\n",
        "# g. Find the position of hypothesis_score in obj_corr_score to get the obj_rank\n",
        "obj_rank_worst = np.sum(np.less_equal(triple_score[0], obj_corr_score)) + 1\n",
        "print('Assigning the worst rank (to break ties):', obj_rank_worst)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2OPFtvC_TJL"
      },
      "source": [
        "print('Subject corruption rank:', sub_rank_worst)\n",
        "print('Object corruption rank:', obj_rank_worst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-d0JAxjC6ie"
      },
      "source": [
        "**Computing the (Unfiltered) rank using evaluate_performance API**\n",
        "\n",
        "We can use the [evaluate_performance](https://docs.ampligraph.org/en/latest/generated/ampligraph.evaluation.evaluate_performance.html) API to compute the ranks. By default, `evaluate_performance` API computes the unfiltered ranks i.e. if any true positives are present in corruptions, they will not be removed before ranking. However, usually for evaluation, we follow a filtered evaluation as described in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06VnAcRyC5gk"
      },
      "source": [
        "from ampligraph.evaluation import evaluate_performance \n",
        "\n",
        "ranks = evaluate_performance(np.array([test_triple]), \n",
        "                             model=model,\n",
        "                             ranking_strategy='worst')\n",
        "\n",
        "print('\\nRanks:', ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgkpA_BWk_uo"
      },
      "source": [
        "There are multiple strategies to compute ranks especially when there are ties. Lets look at each of them in detail with an example. \n",
        "\n",
        "Assume there are only 10 corruptions, and assume that all the corruptions get the same score as the test triple. The ranks are as follows \n",
        "- Assign the **worst rank** i.e. the test set triple gets a rank of 11. This is followed by most papers in the literature. This is the strictest approach and it drives down the mrr by a large margin if there are many ties. We employ this strategy in AmpliGraph.\n",
        "\n",
        "<center> $rank = COUNT( corruption_{score} \\ge hypothesis_{score} )$ + 1</center>\n",
        "    \n",
        "- Assign the **middle rank** i.e. the test set triple gets a rank of 6. We found this strategy being used by [ICLR 2020 paper](https://openreview.net/pdf?id=BkxSmlBFvr). This approach seems to be fair towards the model in resolving the ties as it assigns the middle rank to break ties.\n",
        "\n",
        "<center> $rank = COUNT( corruption_{score} \\gt hypothesis_{score} ) + \\dfrac{COUNT( corruption_{score} == hypothesis_{score} )}{2}$ + 1</center>\n",
        "\n",
        "- Assign the **best rank** i.e. the test set triple gets a rank of 1. This approach is followed by [ConvKB paper](https://arxiv.org/pdf/1712.02121.pdf).  This approach is overly biased and helps the model achieve a very good mrr in case of ties.\n",
        "\n",
        "<center> $rank = COUNT( corruption_{score} \\gt hypothesis_{score} )$ + 1</center>\n",
        "\n",
        "We recommend the usage of the **worst** strategy (default)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuJpqTDklDux"
      },
      "source": [
        "## 3.2 Filtered evaluation\n",
        "While evaluating ([as described earlier](#Compute-the-evaluation-metrics)), we generate all the corruptions (using all the unique entities in our dataset) per test triple, score and rank them. While doing so, we are not filtering the true positives - in other words, some of the corruptions may not really be corruptions and may be ground truth triples observed during training. Training triples usually get a high score as they are \"observed\" by the model. Hence a test triple would get a lower rank if such triples appear in corruptions. To filter out the True Positives (after step b. and e.), one can pass all the True Positive triples  to `filter_triples` parameter of the `evaluate_performance` API. This will perform a **\"filtered\" evaluation** and return the **\"filtered\" ranks** adjusted by removing the True Positives from the corruptions. More details for `evaluate_performance` API can be found [here](https://docs.ampligraph.org/en/latest/generated/ampligraph.evaluation.evaluate_performance.html#ampligraph.evaluation.evaluate_performance).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLD9MgxkC5Oo"
      },
      "source": [
        "from ampligraph.evaluation import evaluate_performance \n",
        "\n",
        "print('Size of X_test:', X_test.shape)\n",
        "\n",
        "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
        "\n",
        "ranks = evaluate_performance(np.array([test_triple]), \n",
        "                             model=model,\n",
        "                             filter_triples=X_filter)\n",
        "\n",
        "print(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdAw7EUMlYon"
      },
      "source": [
        "One obvious question is why do we append the Valid and Test set to the X_filter. The model has not \"observed\" them during training. We do so because, we would like to evaluate a test triple against it's corruptions and not against known facts. If we know that the Validation triples and Test triples are facts (and not queries), we need to filter these triples out of the generated corruptions. This is the standard procedure that is used to compute the metrics to compete on the leadership board."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_YEKhYglae3"
      },
      "source": [
        "## 3.3 Aggregate metrics\n",
        "\n",
        "\n",
        "Once we have the ranks for all the test set triples, we can compute the following aggregate metrics: **MR**, **MRR**, **Hits@N**. These metrics indicate the overall quality of the model on a test set. These metrics come from Information Retrieval domain and are always computed on a set of **True Statements**. To illustrate each of these metric let us first create a small test set of 5 triples and compute their ranks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXe0FgPeC_Si"
      },
      "source": [
        "X_test_small = np.array(\n",
        "                [['doctorate',\n",
        "                    '/education/educational_degree/people_with_this_degree./education/education/major_field_of_study',\n",
        "                    'computer engineering'],\n",
        "\n",
        "                ['star wars',\n",
        "                    '/film/film/estimated_budget./measurement_unit/dated_money_value/currency',\n",
        "                    'united states dollar'],\n",
        "\n",
        "                ['harry potter and the chamber of secrets',\n",
        "                    '/film/film/estimated_budget./measurement_unit/dated_money_value/currency',\n",
        "                    'united states dollar'],\n",
        "\n",
        "                ['star wars', '/film/film/language', 'english language'],\n",
        "                ['harrison ford', '/film/actor/film./film/performance/film', 'star wars']])\n",
        "\n",
        "\n",
        "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
        "\n",
        "ranks = evaluate_performance(X_test_small, \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter, \n",
        "                             corrupt_side='s,o')\n",
        "print(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKy0b0XVlgRY"
      },
      "source": [
        "Now let us look at each aggregate metrics in detail:\n",
        "\n",
        "- **Mean rank (MR)**, as the name indicates, is the average of all the ranks of the triples. The value ranges from 1 (ideal case when all ranks equal to 1) to number of corruptions (where all the ranks are last).\n",
        "\n",
        "![mr formula](https://user-images.githubusercontent.com/281477/90627586-105d6180-e214-11ea-84d4-c5d3e4b089f4.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aaCDgkFldn6"
      },
      "source": [
        "from ampligraph.evaluation import mr_score\n",
        "print('MR :', mr_score(ranks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDyhtqxYYMmv"
      },
      "source": [
        "- **Mean reciprocal rank (MRR)**, is the average of the reciprocal ranks of all the triples. The value ranges from 0 to 1; higher the value better is the model.\n",
        "\n",
        "![mrr formula](https://user-images.githubusercontent.com/281477/90627604-12272500-e214-11ea-9777-5d30b23f0d6f.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8El8j03AYL5d"
      },
      "source": [
        "from ampligraph.evaluation import mrr_score\n",
        "print('MRR :', mrr_score(ranks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcw8Sgc3lmPc"
      },
      "source": [
        "MRR is an indicator of mean rank after removing the effect of outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbWTp59JlnaH"
      },
      "source": [
        "print('Mean rank after removing the outlier effect: ', np.ceil(1/mrr_score(ranks)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d_YGuEJlpxw"
      },
      "source": [
        "- **hits@n** is the percentage of computed ranks that are greater than (in terms of ranking) or equal to a rank of n. The value ranges from 0 to 1; higher the value better is the model.\n",
        "\n",
        "![hits formula](https://user-images.githubusercontent.com/281477/90627565-09365380-e214-11ea-81c8-292a3de016d0.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5YHZs9elojV"
      },
      "source": [
        "from ampligraph.evaluation import hits_at_n_score\n",
        "print('hits@1 :', hits_at_n_score(ranks, 1))\n",
        "print('hits@10 :', hits_at_n_score(ranks, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzvhHy6XIHXI"
      },
      "source": [
        "# print unique entities\n",
        "print('Number of unique entities:', len(model.ent_to_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzUfheLiOfwU"
      },
      "source": [
        "**What if, for a model, you observe that on a test set, the MRR score is 0.01? Is it a good model?**\n",
        "\n",
        "It is not very straightforward. What the above value means is that if you remove the outlier effect, on an average the ranks are around 100 (1/0.01). It may be a good/bad value. It depends on number of corruptions that you have used for the computation. Say you had 1 million corruptions and yet the mrr score was 0.01. The model, in general, was quite good at ranking against 1 million corruption because on an average it gave a rank of close to 100. But say if the corruptions were only 100 and we had an mrr of 0.01, it means that the model did a very bad task at ranking the test triples against just 100 corruptions.\n",
        "\n",
        "On a real life dataset, on should take a closer look at **hits@n** values and decide whether the model is a good model or not. ***The choice of n should depend on the number of corruptions that are being generated per test triple***. If a large percentage of ranks computed on the test set triple falls within the n ranks, then the model can be considered as a good model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLxF6F1Blxf1"
      },
      "source": [
        "\n",
        "def display_aggregate_metrics(ranks):\n",
        "    print('Mean Rank:', mr_score(ranks)) \n",
        "    print('Mean Reciprocal Rank:', mrr_score(ranks)) \n",
        "    print('Hits@1:', hits_at_n_score(ranks, 1))\n",
        "    print('Hits@10:', hits_at_n_score(ranks, 10))\n",
        "    print('Hits@100:', hits_at_n_score(ranks, 100))\n",
        "\n",
        "\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM0ax6rPpbpe"
      },
      "source": [
        "\n",
        "## 3.4. Training with early stopping\n",
        "\n",
        "While training a model, we would like to make sure that the model does not overfit or under fit on the data. If we train a model for a fixed number of epochs, we will not know whether the model has underfit or overfit the training data. Hence it is necessary to test the model performance on a held out set at regular intervals to decide when to stop training. This is called \"Early stopping\", i.e. we don't let the model run for a long time but stop much before when the performance on the held out set starts to degrade. \n",
        "\n",
        "However we also do not want to model to overfit on the held out set and limit the generalization capabilities of the model. Hence we should create both a validation set and a test set to verify the generalization capability of the model, and to make sure that we dont over fit and under fit on the data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DagdzuwspU1Q"
      },
      "source": [
        "early_stopping_params = { 'x_valid': X_valid,   # Validation set on which early stopping will be performed\n",
        "                          'criteria': 'mrr',    # metric to watch during early stopping\n",
        "                          'burn_in': 150,       # Burn in time, i.e. early stopping checks will not be performed till 150 epochs\n",
        "                          'check_interval': 50, # After burn in time, early stopping checks will be performed at every 50th epochs (i.e. 150, 200, 250, ...)\n",
        "                          'stop_interval': 2,   # If the monitored criteria degrades for these many epochs, the training stops. \n",
        "                          'corrupt_side':'s,o'  # Which sides to corrupt furing early stopping evaluation (default both subject and obj as described earlier)\n",
        "                        }\n",
        "\n",
        "# create a model as earlier\n",
        "model = TransE(k=100, \n",
        "               epochs=10, \n",
        "               eta=1, \n",
        "               loss='multiclass_nll', \n",
        "               initializer='xavier', initializer_params={'uniform': False},\n",
        "               regularizer='LP', regularizer_params= {'lambda': 0.0001, 'p': 3},\n",
        "               optimizer= 'adam', optimizer_params= {'lr': 0.001}, \n",
        "               seed= 0, batches_count= 1, verbose=True)\n",
        "\n",
        "# call model.fit by passing early stopping params\n",
        "model.fit(X_train,                                      # training set\n",
        "          early_stopping=True,                          # set early stopping to true\n",
        "          early_stopping_params=early_stopping_params)  # pass the early stopping params\n",
        "\n",
        "# evaluate the model with filter\n",
        "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
        "ranks = evaluate_performance(X_test, \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter)\n",
        "# display the metrics\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic7r20ScpS78"
      },
      "source": [
        "\n",
        "## Summary so far\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBXrmiA1lzjO"
      },
      "source": [
        "# ----------------------\n",
        "# Generate train/test data\n",
        "# create train/test/valid splits, train the model and evaluate using train_test_split_no_unseen API\n",
        "from ampligraph.evaluation import train_test_split_no_unseen\n",
        "# get the validation set of size 500\n",
        "test_train, X_valid = train_test_split_no_unseen(dataset.values, 500, seed=0)\n",
        "\n",
        "# get the test set of size 1000 from the remaining triples\n",
        "X_train, X_test = train_test_split_no_unseen(test_train, 1000, seed=0)\n",
        "# ----------------------\n",
        "# Training:\n",
        "\n",
        "print('Training set:', X_train.shape)\n",
        "\n",
        "# Train a KGE model\n",
        "model = TransE(k=300, \n",
        "               epochs=100, \n",
        "               eta=1, \n",
        "               loss='multiclass_nll', \n",
        "               initializer='xavier', initializer_params={'uniform': False},\n",
        "               regularizer='LP', regularizer_params= {'lambda': 0.001, 'p': 3},\n",
        "               optimizer= 'adam', optimizer_params= {'lr': 0.0001}, \n",
        "               seed= 0, batches_count= 10, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "# ----------------------\n",
        "# Evaluate: \n",
        "# Filtered evaluation with ranking strategy assigning worst rank to break ties\n",
        "\n",
        "from ampligraph.utils import save_model, restore_model\n",
        "save_model(model, 'TransE.pkl')\n",
        "model = restore_model('TransE.pkl')\n",
        "\n",
        "# create the filter \n",
        "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
        "\n",
        "# compute ranks\n",
        "ranks = evaluate_performance(X_test, \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter)\n",
        "\n",
        "# ranks are computed per triple\n",
        "print('Test set:', X_test.shape)\n",
        "print('Size of ranks:', ranks.shape)\n",
        "\n",
        "# Aggregate metrics show the aggregate performance of the model on the test set using a single number\n",
        "display_aggregate_metrics(ranks)\n",
        "# ----------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP2K-mHgQ54K"
      },
      "source": [
        "##**Key Takeaways**\n",
        "- `train_test_split_no_unseen` API can be used to generate train/test splits such that test set contains only entities 'seen' during training\n",
        "- Once a model is trained, one can use `model.predict` to choose from a set of hypothesis based on the scores returned by the model.\n",
        "- One can access the quality of model on a **test set of True Facts** by using metrics such as MR, MRR and hits@n\n",
        "- We can use early stopping to prevent model from over/under fitting by using a Validation Set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjm3VWG8dzna"
      },
      "source": [
        "---\n",
        "##Q&A\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0NmjxkhiMV5"
      },
      "source": [
        "# 4. Practical evaluation protocols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dTaNLGeiXCL"
      },
      "source": [
        "Standard protocols, as described earlier, follow a very strict way of evaluating the test set. We corrupt both the subject and object sides with all the entities present in the KG. Also, when the KG is huge with millions of entities, the standard protocol is not feisible. Due to the large number of corruptions, some of which may be semantically incorrect, it becomes a difficult task for the model while ranking and it may lead to misleading metrics. Hence the standard protocol is not recommended for large KGs. \n",
        "\n",
        "Let's now look at some practical ways of evaluating for large KGs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ibg5h02pYnb"
      },
      "source": [
        "\n",
        "\n",
        "## 4.1 Evaluating by corrupting specific sides\n",
        "Let's assume that our test set is made up of triples of type <movie, film_language, language_category> and we want to find if our model can correctly find the language of the movie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emb5qFfmpZRW"
      },
      "source": [
        "X_test_movie_languages = X_test[X_test[:, 1] == '/film/film/language']\n",
        "X_test_movie_languages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsBcVe5qpnGU"
      },
      "source": [
        "With the evaluation shown below, we are using all entities in our dataset and corrupting both subject and object sides of the test triple and returning 2 ranks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PAAByzfpZ9c"
      },
      "source": [
        "ranks = evaluate_performance(X_test_movie_languages, \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter)\n",
        "\n",
        "display_aggregate_metrics(ranks)\n",
        "print('\\nSize of test set:', X_test_movie_languages.shape)\n",
        "print('Size of ranks:', ranks.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN6cFPOxprzp"
      },
      "source": [
        "This is because [evaluate performance](https://docs.ampligraph.org/en/latest/generated/ampligraph.evaluation.evaluate_performance.html#ampligraph.evaluation.evaluate_performance) with default protocol does the following:\n",
        "- computes rank by corrupting the subject side (`'s'`) \n",
        "- computes rank by corrupting the object side (`'o'`) \n",
        "- returns both the ranks per triple.\n",
        "\n",
        "The metrics (such as mrr, mr, hits@n) are computed by flattening and averaging the ranks.\n",
        "\n",
        "This is the standard protocol that is usually followed while doing graph completion and is usually adopted for computing the metrics (on traditional datasets like freebase or wordnet) while competing on the leadership board.\n",
        "\n",
        "If we want to corrupt specific sides (to suit our use-case), we can do so by passing `corrupt_side` parameter to `evaluate_performance`. It can take on the following values:\n",
        "- `s` for subject corruption only\n",
        "- `o` for object corruption only\n",
        "- `s+o` for subject and object corruption. Returns a single rank.\n",
        "- `s,o` for subject and object corruption separately (default). Returns 2 ranks. This is equivalent to calling `evaluate_performance` twice with `s` and `o`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH_hLOgippsP"
      },
      "source": [
        "ranks = evaluate_performance(X_test_movie_languages, \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter,\n",
        "                             corrupt_side='o')\n",
        "\n",
        "display_aggregate_metrics(ranks)\n",
        "print('\\nSize of test set:', X_test_movie_languages.shape)\n",
        "print('Size of ranks:', ranks.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEodiLjtpvrl"
      },
      "source": [
        "As you see, only 1 rank is returned per triple, and this rank is the rank obtained by corrupting only the specified side with all the entities in the KG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4CTkEakpx_Z"
      },
      "source": [
        "## 4.2 Evaluating against a subset of entities\n",
        "\n",
        "Depending on the use case or size of the graph, you may want to evaluate the test set by generating corruptions only from a subset of entities. This can be done by using `entities_subset` argument. For example, let's say we are doing a genetic study using KG. The graph may have different entity types like patient, diseases, genes, mutations, co-morbidities,ect. Say we want to find out what mutations cause disease i.e. `< ?, causes, disease_name>`. For this use case it doesnt make sense to replace the placeholder with all the entities. A logical replacement would be by using all the mutations.\n",
        "\n",
        "Similarly for our use case, we are interested in finding the language of the movie. So it makes sense to use only language categories to generate the corruptions for the object side. It also makes the task easier for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd9_hhfApuGt"
      },
      "source": [
        "print('The number of corruptions generated per triple is:', len(model.ent_to_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUHoxk1op1B9"
      },
      "source": [
        "unique_languages = set(X_train[X_train[:, 1] == '/film/film/language'][:, 2])\n",
        "\n",
        "print('Number of languages in KG:', len(unique_languages))\n",
        "print('\\n', unique_languages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqqfHUeGp1t9"
      },
      "source": [
        "ranks = evaluate_performance(X_test_movie_languages, \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter,\n",
        "                             corrupt_side='o',\n",
        "                             entities_subset=list(unique_languages))\n",
        "\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we5ChCYAp4CU"
      },
      "source": [
        "Usually, we can see a drastic increase in the metric values mainly because we are using fewer **semantically \"valid\" corruptions**. \n",
        "\n",
        "When we have a schema for our KG, and the focus is not just graph completion but a specific use case (Eg: similar job search, product recommendation, gene discovery to target a disease, etc), we would  recommend using semantically \"valid\" corruptions by looking at the schema to do performance evaluation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dU-E_bUSoGm"
      },
      "source": [
        "##**Key Takeaways**\n",
        "- During default evaluation, the `evaluate_performance` API corrupts both subject and object side with ALL the entities in the KG and returns 2 ranks. This is a hard task for the model and the resulting metrics may mislead the user. \n",
        "- Depending on use case, we can corrupt specific sides of the triples by specifying the `corrupt_side` argument; and we can also provide semantically valid entities to be used for generating corruptions by using the `entities_subset` argument in `evaluate_performance` API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHGc1fulp28L"
      },
      "source": [
        "---\n",
        "# 5. Comparision of Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8eyZE5hqEkt"
      },
      "source": [
        "from ampligraph.latent_features import TransE, ComplEx, HolE, DistMult, ConvE, ConvKB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlXFlocUqGQX"
      },
      "source": [
        "## 5.1 Traditional models :\n",
        "\n",
        "These models take as input vector representation of embeddings of entities and predicates of a triple. The embeddings are combined using a scoring function to generate a score. Ranking protocol is followed to train/evaluate the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV2-4pS_l54F"
      },
      "source": [
        "### TransE\n",
        "This is one of the first embedding models which set the platform for the KGE research. It uses simple vector algebra to score the triples. It has very low number of trainable parameters compared to most models. \n",
        "\n",
        "<center>$f = - || s + p - o ||_{n}$</center>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPRuQ93iqIwV"
      },
      "source": [
        "model = TransE(k=150, epochs=50, eta=1, loss='multiclass_nll', \n",
        "               initializer='xavier', initializer_params={'uniform': False},\n",
        "               regularizer='LP', regularizer_params= {'lambda': 0.0001, 'p': 3},\n",
        "               optimizer= 'adam', optimizer_params= {'lr': 0.001}, \n",
        "               seed= 0, batches_count= 1, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "ranks = evaluate_performance(X_test, \n",
        "                             model=model,\n",
        "                             filter_triples=X_filter,\n",
        "                             corrupt_side='s,o',\n",
        "                             ranking_strategy='worst')\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wEGdaVtqN00"
      },
      "source": [
        "print('The number of unique entities:', len(model.ent_to_idx))\n",
        "print('The number of unique relations:', len(model.rel_to_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EO10ZV9qOsX"
      },
      "source": [
        "print('Size of entity embeddings:', model.ent_emb.shape)\n",
        "print('Size of entity embeddings:', model.rel_emb.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTE3qGJ-qPcX"
      },
      "source": [
        "###DistMult\n",
        "This model is similar to TransE. Instead of additive operations, it does multiplication of vectors to compute the score. DistMult also has same number of parameters as TransE. This model is **quite bad at differenciating anti-symmetric relations** (especially during knowledge discovery), because <s,p,o> and <o,p,s> would get the same score.\n",
        "\n",
        "<center>$f = \\sum s * p * o$</center>\n",
        "\n",
        "Example: \\< Jack Likes VideoGames \\> \\< VideoGames Likes Jack \\>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9TSQcLRqRJ9"
      },
      "source": [
        "model = DistMult(k=150, epochs=50, eta=1, loss='multiclass_nll', \n",
        "                initializer='xavier', initializer_params={'uniform': False},\n",
        "                regularizer='LP', regularizer_params= {'lambda': 0.0001, 'p': 3},\n",
        "                optimizer= 'adam', optimizer_params= {'lr': 0.001}, \n",
        "                seed= 0, batches_count= 1, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "ranks = evaluate_performance(X_test, \n",
        "                             model=model,\n",
        "                             filter_triples=X_filter,\n",
        "                             corrupt_side='s,o',\n",
        "                             ranking_strategy='worst')\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4-Y-I8eqSpu"
      },
      "source": [
        "print('Size of entity embeddings:', model.ent_emb.shape)\n",
        "print('Size of entity embeddings:', model.rel_emb.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgFKn4CRqTzX"
      },
      "source": [
        "###Complex\n",
        "This model can be thought of as performing DistMult like operations but in Complex space. The number of parameters is twice as that of TransE and DistMult (k for real part and k for imaginary part). The scoring function can handle symmetry and anti-symmetry quite well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pPOqJ1-qV46"
      },
      "source": [
        "model = ComplEx(k=150, epochs=50, eta=1, loss='multiclass_nll', \n",
        "                initializer='xavier', initializer_params={'uniform': False},\n",
        "                regularizer='LP', regularizer_params= {'lambda': 0.0001, 'p': 3},\n",
        "                optimizer= 'adam', optimizer_params= {'lr': 0.001}, \n",
        "                seed= 0, batches_count= 1, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "ranks = evaluate_performance(X_test, \n",
        "                             model=model,\n",
        "                             filter_triples=X_filter,\n",
        "                             corrupt_side='s,o',\n",
        "                             ranking_strategy='worst')\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIrjD-dPqWoN"
      },
      "source": [
        "print('Size of entity embeddings:', model.ent_emb.shape)\n",
        "print('Size of entity embeddings:', model.rel_emb.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u_YxZGRqYWk"
      },
      "source": [
        "## 5.2 Convolutional models\n",
        "\n",
        "These are convolutional models. They converts embeddings to an \"image\" like representation, and performs convolutions on them. Instead of embedding vectors(for s, p and o) as inputs to the model, you can think of the inputs to be like a 2 or 3-channel image where each channel represents s, p and o features.\n",
        "\n",
        "Both models are similar in terms of their architecture, that is while extracting feature representation of inputs; but the main difference lies in the way in which the corruptions are generated and how the loss is computed. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xP_BA1Gmto3"
      },
      "source": [
        "### ConvKB\n",
        "\n",
        "ConvKB generates eta corruptions per training triple and computes feature matrix for the triples/corruptions (s,p,o) using shared layers. It uses margin based losses like other KGE models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f5wQM8Lqapv"
      },
      "source": [
        "model = ConvKB(k=150, epochs=50, eta=1, loss='multiclass_nll', \n",
        "                initializer='xavier', initializer_params={'uniform': False},\n",
        "                regularizer='LP', regularizer_params= {'lambda': 0.0001, 'p': 3},\n",
        "                optimizer= 'adam', optimizer_params= {'lr': 0.001}, \n",
        "                seed= 0, \n",
        "                batches_count= 5, # Goes OOM (ResourceExhaustedError) if batch count is 1\n",
        "                verbose=True)\n",
        "\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "ranks = evaluate_performance(X_test, \n",
        "                             model=model,\n",
        "                             filter_triples=X_filter,\n",
        "                             corrupt_side='s,o',\n",
        "                             ranking_strategy='worst')\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65mg1RtdqcsW"
      },
      "source": [
        "print('Size of entity embeddings:', model.ent_emb.shape)\n",
        "print('Size of entity embeddings:', model.rel_emb.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d62IQMi4m95u"
      },
      "source": [
        "###ConvE\n",
        "ConvE on the other hand does a \"target\" prediction. It uses s and p embeddings and tries to predict all the o's (multi-hot encoding) in the graph. In one way, it is equivalent to treating all the unknown entities during training as a negative. Due to this approach, one needs to use the standard \"target-based\" losses for training ConvE models.\n",
        "\n",
        "\n",
        "The output layer of ConvE is extremely huge (since it is equal to the number of unique entities in the graph), which in turn results in scalability issues when working with practical graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jev0TITPqfFg"
      },
      "source": [
        "model = ConvE(k=150, epochs=2, loss='bce', \n",
        "                initializer='xavier', initializer_params={'uniform': False},\n",
        "                regularizer='LP', regularizer_params= {'lambda': 0.001, 'p': 3},\n",
        "                optimizer= 'adam', optimizer_params= {'lr': 0.001}, \n",
        "                seed= 0, batches_count= 20, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "ranks = evaluate_performance(X_test, \n",
        "                             model=model,\n",
        "                             filter_triples=X_filter,\n",
        "                             corrupt_side='o',\n",
        "                             ranking_strategy='worst')\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75svVDO5qgMo"
      },
      "source": [
        "print('Size of entity embeddings:', model.ent_emb.shape)\n",
        "print('Size of entity embeddings:', model.rel_emb.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GskySRoZ-dG"
      },
      "source": [
        "###**Key Takeaways**\n",
        "- Traditional models use vector representation of embeddings as inputs, where as Convolutional models use image-like representations and perform convolutions on them.\n",
        "- Convolutional models are extremely good when it comes to performance on standard datasets, however they don't scale well as the dataset size increases.\n",
        "- ComplEx model uses 2*k embeddings internally (for real and imaginary parts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaQu5Im9q0ng"
      },
      "source": [
        "---\n",
        "# 6. Hyperparameter Selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIKd1v3xodV7"
      },
      "source": [
        "## 6.1 Role of Hyperparameters\n",
        "\n",
        "A large value of ***k*** may result in overfitting, and the size of the embeddings on the disk would also be large. It may also happen that the embedding matrix may not even fit on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpQi2r6Oq4on"
      },
      "source": [
        "from ampligraph.latent_features import TransE\n",
        "\n",
        "model = TransE(k=1000, epochs=20, eta=1, loss='multiclass_nll', \n",
        "                initializer='xavier', initializer_params={'uniform': False},\n",
        "                regularizer='LP', regularizer_params= {'lambda': 0.001, 'p': 3},\n",
        "                optimizer= 'adam', optimizer_params= {'lr': 0.001}, \n",
        "                seed= 0, batches_count= 5, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "ranks = evaluate_performance(X_test[::10], \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter, \n",
        "                             corrupt_side='s,o')\n",
        "\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oANdethq7OX"
      },
      "source": [
        "A small ***k*** may result in underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVSMp1_xq8-h"
      },
      "source": [
        "model = TransE(k=10, epochs=20, eta=1, loss='multiclass_nll', \n",
        "                initializer='xavier', initializer_params={'uniform': False},\n",
        "                regularizer='LP', regularizer_params= {'lambda': 0.001, 'p': 3},\n",
        "                optimizer= 'adam', optimizer_params= {'lr': 0.001}, \n",
        "                seed= 0, batches_count= 5, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "ranks = evaluate_performance(X_test[::10], \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter, \n",
        "                             corrupt_side='s,o')\n",
        "\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ1_Hqj4rBV4"
      },
      "source": [
        "Ideally, you should choose a ***k*** which is large enough, along with a proper choice of ***eta***. \n",
        "\n",
        "It has been observed that the performance of model increases with increase in ***eta*** up to a certain point and then saturates. Usually a good value for eta is between 20-30. [add reference]()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEK2TAYJrDxP"
      },
      "source": [
        "model = TransE(k=250, epochs=20, eta=20, loss='multiclass_nll', \n",
        "                initializer='xavier', initializer_params={'uniform': False},\n",
        "                regularizer='LP', regularizer_params= {'lambda': 0.001, 'p': 3},\n",
        "                optimizer= 'adam', optimizer_params= {'lr': 0.001}, \n",
        "                seed= 0, batches_count= 10, verbose=True)\n",
        "\n",
        "model.fit(X_train)\n",
        "\n",
        "ranks = evaluate_performance(X_test[::10], \n",
        "                             model=model, \n",
        "                             filter_triples=X_filter, \n",
        "                             corrupt_side='s,o')\n",
        "display_aggregate_metrics(ranks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j8UBMtLxQ5L"
      },
      "source": [
        "\n",
        "## 6.2 Grid Search and Random Search\n",
        "\n",
        "Ampligraph provides an API to perform model selection and to run experimental campaigns on datasets. One can use [select_best_model_ranking](https://docs.ampligraph.org/en/latest/generated/ampligraph.evaluation.select_best_model_ranking.html) to perform model selection. It supports grid search and random search.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvNf4pJWrIxA"
      },
      "source": [
        "from ampligraph.evaluation import select_best_model_ranking\n",
        "\n",
        "model_class = TransE\n",
        "param_grid = {\n",
        "                     \"batches_count\": [5],\n",
        "                     \"seed\": 0,\n",
        "                     \"epochs\": [50],\n",
        "                     \"k\": [150, 50],\n",
        "                     \"eta\": [1, 5],\n",
        "                     \"loss\": [\"multiclass_nll\"],\n",
        "                     \"loss_params\": {},\n",
        "                     \"embedding_model_params\": {},\n",
        "                     \"regularizer\": [\"LP\"],\n",
        "                     \"regularizer_params\": {\n",
        "                         \"p\": [3],\n",
        "                         \"lambda\": [1e-3]\n",
        "                      },\n",
        "                     \"optimizer\": [\"adam\"],\n",
        "                     \"optimizer_params\":{\n",
        "                         \"lr\": 0.001 #lambda: np.random.uniform(0.00001, 0.01)\n",
        "                     },\n",
        "                     \"verbose\": False\n",
        "                 }\n",
        "best_model, best_params, best_mrr_train, ranks_test, mrr_test, experimental_history = \\\n",
        "        select_best_model_ranking(model_class, \n",
        "                          X_train, \n",
        "                          X_valid, \n",
        "                          X_test, \n",
        "                          param_grid,\n",
        "                         max_combinations=2, # performs random search-executes 2 models by randomly choosing params\n",
        "                          use_filter=True, \n",
        "                          verbose=True,\n",
        "                          early_stopping=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT2aa3DyrMB0"
      },
      "source": [
        "print('MRR of the best model:', best_mrr_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgBRSXN7rM-P"
      },
      "source": [
        "# params of the best model\n",
        "best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di_zQzkjrPJ5"
      },
      "source": [
        "You can also look at the experimental history and check the various combinations tested during the model selection, along with the results for each combination, using the experimental history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pOHp0UNrP6Y"
      },
      "source": [
        "experimental_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfKJdvRpb9co"
      },
      "source": [
        "##**Key Takeaways**\n",
        "- Large value of k may increase the model performance slightly, however it may result in OOM on GPU and it will need larger storage space on disk.\n",
        "- A small k would result in model underfitting the data\n",
        "- Ideal option is to choose an \"in-between\" k and an appropriate value for eta. A good choice will result is similar mrr as that obtained with larger k.\n",
        "- Rather than running multiple models manually for hyperparameter selection, one can use the `select_best_model_ranking` API provided by AmpliGraph to do model selection using either grid search or random search.\n",
        "- You can use a callable in the search param grid and specify `max_combinations` parameter in the `select_best_model_ranking` API. AmpliGraph will perform random search, by randomly choosing values from the callable. It will evaluate `max_combinations` number of models and return the best one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uK5o7lrrQ1R"
      },
      "source": [
        "---\n",
        "# 7. Model Calibration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj0IIXk-rTRq"
      },
      "source": [
        "model = restore_model('TransE.pkl')\n",
        "X_test_small = np.array([['star wars', '/film/film/language', 'english language'],\n",
        "                         ['star wars', '/film/film/language', 'java']]) \n",
        "\n",
        "model.predict(X_test_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC92WqR8rVNR"
      },
      "source": [
        "As described earlier, model.predict returns a score which doesn't signify anything because the score is not bound for most of the models. It is just a value and to interpret it we use the ranking protocol.\n",
        "\n",
        "However, one can also [calibrate](https://docs.ampligraph.org/en/latest/generated/ampligraph.latent_features.EmbeddingModel.html?#ampligraph.latent_features.EmbeddingModel.calibrate) the scores of a model, so that one can get a bounded confidence estimate which ranges from 0 to 1. This is done by performing a logistic regression on the score of triples. One can use the `calibrate` API to do this. It takes an argument `X_Pos` which should be True Positives (Eg. training set). If a list of True Negatives are available, then this can be passed to `X_Neg`. The model would calibrate the scores by tuning a logistic regressor. One can then use `predict_proba` API to get a bounded score. \n",
        "\n",
        "If a list of True Negatives are not available, then the calibration can be performed on synthetic corruptions. However, while doing so, one must pass the `positive_base_rate` argument which specifies the base rate of positive statements. Choosing this value is a challenging task and it affects the value predicted by `predict_proba`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jteRatPrWPI"
      },
      "source": [
        "model.calibrate(X_train, \n",
        "                X_neg=None, \n",
        "                positive_base_rate=0.5, \n",
        "                batches_count=100, \n",
        "                epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v8qG-3MrXQZ"
      },
      "source": [
        "model.predict_proba(X_test_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ-7cLsBrbTm"
      },
      "source": [
        "For more details on calibration refer [this paper](https://arxiv.org/abs/1912.10000)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbWs92Bxoe_E"
      },
      "source": [
        "**Key Takeaways:**\n",
        "- Models usually return a score between +inf and -inf (depending on the type). These scores can be calibrated to the range [0, 1] using `model.calibrate`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdX1lwK4rX_Y"
      },
      "source": [
        "---\n",
        "# 8. Knowledge Discovery \n",
        "\n",
        "In Ampligraph we provide a number of high-level convenience functions for performing knowledge discovery using graph embeddings:\n",
        "\n",
        "> ***query_topn***: which when given two elements of a triple will return the top_n results of all possible completions ordered by predicted score.\n",
        "\n",
        "> ***discover_facts***: generate a set of candidate statements using one of several defined strategies and return triples that perform well when evaluated against corruptions.\n",
        "\n",
        "> ***find_clusters***: perform link-based cluster analysis on graph embeddings.\n",
        "\n",
        "> ***find_duplicates***: which will find duplicate entities in a graph based on their embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3dmkTgHrdB0"
      },
      "source": [
        "## 8.1 Triple completion\n",
        "\n",
        "Sometimes you may have either a relation and entity (head or tail) pair, or just two entities, and you want to see what the top n results returned by the model are that completes the triple. \n",
        "\n",
        "``` \n",
        "    <head, relation, ?> \n",
        "    <head, ?,        tail>\n",
        "    <?,    relation, tail>\n",
        "```\n",
        "\n",
        "Specify ```rels_to_consider``` or ```ents_to_consider``` lists to return triples where the missing element is filled only from that list. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U1Wur_hravz"
      },
      "source": [
        "from ampligraph.discovery import query_topn\n",
        "\n",
        "# restore the previously saved model to save time\n",
        "model = restore_model('TransE.pkl')\n",
        "\n",
        "triples, scores = query_topn(model, top_n=10, \n",
        "                             head='missy elliott', \n",
        "                             relation='/people/person/profession', \n",
        "                             tail=None, \n",
        "                             ents_to_consider=None, \n",
        "                             rels_to_consider=None)\n",
        "\n",
        "for triple, score in zip(triples, scores):\n",
        "    print('Score: {} \\t {} '.format(score, triple))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i8J-rFvIPuB"
      },
      "source": [
        "triples, scores = query_topn(model, top_n=10, \n",
        "                             head='the departed', \n",
        "                             relation=None, \n",
        "                             tail='/m/086k8', \n",
        "                             ents_to_consider=None, \n",
        "                             rels_to_consider=None)\n",
        "\n",
        "for triple, score in zip(triples, scores):\n",
        "    print('Score: {} \\t {} '.format(score, triple))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WguBtOZBXnZy"
      },
      "source": [
        "---\n",
        "## 8.2 Clustering\n",
        "\n",
        "Once the model is trained, we can use the embeddings and perform downstream tasks like clustering or classification. Here we will illustrate how to do node clustering. Ampligraph provides an api [find_clusters](https://docs.ampligraph.org/en/latest/generated/ampligraph.discovery.find_clusters.html) which takes in model, the concepts to cluster, and the clustering model (sklearn based). It performs clustering and returns the cluster indices for the concepts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjk5MQfFXleu"
      },
      "source": [
        "from ampligraph.discovery import find_clusters\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# restore the previously saved model to save time\n",
        "model = restore_model('TransE.pkl')\n",
        "\n",
        "# Get the entities that we want to cluster. Here we use all unique entities\n",
        "all_entities = np.array(list(set(dataset.values[:, 0]).union(dataset.values[:, 2])))\n",
        "print('Size of the subset being used for subset generation:', len(all_entities))\n",
        "\n",
        "# create the clustering algorithm from sklearn\n",
        "kmeans = KMeans(n_clusters=3, n_init=100, max_iter=500)\n",
        "\n",
        "# call find_clusters to get the cluster assignments of the entities\n",
        "clusters = find_clusters(all_entities, model, kmeans, mode='entity')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3m3yAuBX-ZN"
      },
      "source": [
        "Now that we have the cluster assignments, let us plot it in a 2d space. Let us use PCA to reduce the dimensions of the embeddings from k=150 to 2 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2WWD6ewkvPM"
      },
      "source": [
        "pip install adjustText==0.7.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H77UV5aJX1RY"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Get the embeddings (150 dims) for all the entities of interest\n",
        "jobs_embeddings = model.get_embeddings(all_entities, embedding_type='entity')\n",
        "\n",
        "# Perform PCA and reduce the dims to 2\n",
        "embeddings_2d = PCA(n_components=2).fit_transform(np.array([emb for emb in jobs_embeddings]))\n",
        "\n",
        "# Create a dataframe to plot the embeddings using scatterplot\n",
        "df = pd.DataFrame({\"entities\": all_entities, \"clusters\": \"cluster\" + pd.Series(clusters).astype(str),\n",
        "                    \"embedding1\": embeddings_2d[:, 0], \"embedding2\": embeddings_2d[:, 1]})\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.title(\"Cluster embeddings\")\n",
        "\n",
        "ax = sns.scatterplot(data=df, x=\"embedding1\", y=\"embedding2\", hue=\"clusters\")\n",
        "\n",
        "# Print only a few labels, to avoid clutter, using adjust_text\n",
        "from adjustText import adjust_text\n",
        "texts = []\n",
        "for i, point in df.iterrows():\n",
        "    # randomly choose a few labels to be printed\n",
        "    if np.random.uniform() < 0.003:\n",
        "        texts.append(plt.text(point['embedding1']+.1, point['embedding2'], str(point['entities'])))\n",
        "        \n",
        "adjust_text(texts)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3I9459RrtED"
      },
      "source": [
        "## 8.3 Hypothesis Generation\n",
        "\n",
        "Other times you may wish to discover **any** potential new facts from an existing knowledge graph. \n",
        "\n",
        "With a knowledge graph containing millions of entities the space of possible facts is huge, and evaluating all of them can take a very long time. In order to speed up this task we have implemented a number of sampling strategies. \n",
        "\n",
        "The strategies implemented include: \n",
        ">    ```entity_frequency```, ```graph_degree```, ```cluster_coefficient```, ```cluster_triangles```, ```cluster_squares```, ```random_uniform```, ```exhaustive```. \n",
        "\n",
        "Entities in all strategies excluding ```random_uniform```, ```exhaustive``` are sorted in ascending fashion, on the assumption that frequent or densely connected entities are less likely to have missing true statements.\n",
        "\n",
        "The general procedure is to generate a set of candidate statements, and then rank them against a set of corruptions using the ```ampligraph.evaluation.evaluate_performance()``` function. \n",
        "\n",
        "A sampling weight is calculated for each entity using the specified strategy, and ```max_candidates``` are sampled to produce the candidate triple set. \n",
        "\n",
        "Candidates are then evaluated to obtain a rank, and triples that appear in the ```top_n``` ranked statements of evaluation procedure are returned as potentially true statements.\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRf_W0ZnrvxC"
      },
      "source": [
        "from ampligraph.discovery import discover_facts\n",
        "\n",
        "triples, ranks = discover_facts(dataset.values, \n",
        "                                model, \n",
        "                                top_n=500, \n",
        "                                max_candidates=500, \n",
        "                                strategy='cluster_triangles', \n",
        "                                target_rel='/people/person/profession', \n",
        "                                seed=42)\n",
        "\n",
        "for triple, rank in zip(triples, ranks):\n",
        "    print('Rank: {} \\t {} '.format(rank, triple))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQS5xvBzrwmy"
      },
      "source": [
        "---\n",
        "# 9. Visualizing embeddings using Tensorboard\n",
        "\n",
        "Tensorboard Projector allows us to visualize high dimensional embeddings in a graphical interface. This can be useful to examine and understand embedded concepts. \n",
        "\n",
        "Ampligraph provides a single function for creating the Tensorboard files, [create_tensorboard_visualizations](https://docs.ampligraph.org/en/latest/generated/ampligraph.utils.create_tensorboard_visualizations.html#ampligraph.utils.create_tensorboard_visualizations), as demonstrated below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3_zf28Zry0k"
      },
      "source": [
        "from ampligraph.utils import create_tensorboard_visualizations\n",
        "\n",
        "model = restore_model('TransE.pkl')\n",
        "\n",
        "create_tensorboard_visualizations(model, 'embeddings_transe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr46N0ANJhTQ"
      },
      "source": [
        "Run the cell below to run tensorboard, and it will open in a different browser window with the tensorboard interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrGF8tcBr5YK"
      },
      "source": [
        "# This will not work in google colab - only uncomment and run if using jupyter notebook \n",
        "\n",
        "! tensorboard --logdir='./embeddings_transe'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMLanHsMsDpN"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "\n",
        "**Assume you are incrementally building knowledge graphs and training models in phases. Can you compare the models trained on these incremental datasets?**\n",
        "\n",
        "Depends. You should have the same test set and same number of corruptions when you want to compare models. If you are only adding new links incrementally, it does not matter. The models would be comparable. However, if you are also adding new concepts, then you must be careful while evaluating the models. You must make sure that the number of corruptions generated are the same throughout. You can choose a set of entities that would be used as corruptions in all the phases and then use `entities_subset` argument of `evaluate_performance` API.\n",
        "\n",
        "\n",
        "**What if my embedding matrices are not fitting on the GPU?**\n",
        "\n",
        "Sometime while running the model, you may run into ResourceExhausted error on the GPU, especially because the batches may not fit in memory. This can usually be solved by increasing the batch count. \n",
        "\n",
        "There are times when you may have millions of entities in the graph, and you may not be able to allocate the embedding matrix on the GPU. In this case, you can use the [large graph mode](https://docs.ampligraph.org/en/latest/dev_notes.html#dealing-with-large-graphs). It gets activated automatically when number of entities is >500000. You can also use `set_entity_threshold` and change this threshold manually. \n",
        "```\n",
        "from ampligraph.latent_features import set_entity_threshold\n",
        "set_entity_threshold(100000)\n",
        "\n",
        "...\n",
        "```\n",
        "\n",
        "In this mode, ampligraph creates the embedding matrix on the CPU and loads only the embeddings of the entities of the batch being trained on the GPU. This mode is much faster than training just on CPU as it can use the GPU cores to speed up computations. More details can be found in the link.\n"
      ]
    }
  ]
}